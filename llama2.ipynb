{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4342e56-bb9e-4521-8361-8572b71cc1fd",
   "metadata": {},
   "source": [
    "https://neurohive.io/en/state-of-the-art/llama-2-and-llama-2-chat/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02bcd97-0386-4709-a32c-81e257bbfb7f",
   "metadata": {},
   "source": [
    "# LLAMA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f257f5d-4bf9-416f-8942-04642470e3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manasabhat/Documents/LLM/llm/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11155c6f-7950-42ec-8ecf-d6eeac443964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLangsmith is a language model designed to assist developers and testers in various ways, including:\\n\\n1. Bug detection: Langsmith can analyze code and identify potential bugs or errors based on its understanding of the programming language and the context of the code.\\n2. Test case generation: Langsmith can generate automated test cases based on the code it analyzes, helping to cover a wider range of scenarios and catch more defects than traditional testing methods.\\n3. Code review: Langsmith can assist in code reviews by identifying potential issues, such as security vulnerabilities or performance bottlenecks, and providing recommendations for improvement.\\n4. Defect prediction: By analyzing code patterns and trends, Langsmith can predict the likelihood of certain types of defects occurring in the future, allowing developers to proactively address potential issues.\\n5. Test data generation: Langsmith can generate test data based on the analysis of the code, helping to ensure that a comprehensive set of tests is executed and increasing the overall efficiency of testing.\\n6. Automated testing: Langsmith can automate testing processes, such as unit testing, integration testing, and regression testing, by analyzing the code and generating test cases based on its understanding of the programming language and the context of the code.\\n7. Testing optimization: Langsmith can help optimize testing efforts by identifying the most critical areas of the code to test, based on the analysis of the code and the potential impact of defects in those areas.\\n8. Defect classification: Langsmith can classify defects based on their severity, priority, and other factors, helping developers to focus their testing efforts on the most critical issues first.\\n9. Testing efficiency improvement: Langsmith can help improve testing efficiency by identifying unnecessary or redundant tests and recommending alternative approaches that are more effective in detecting defects.\\n10. Continuous testing: Langsmith can assist in continuous testing by analyzing code changes as they occur and generating test cases based on the updated code, ensuring that the testing process is kept up to date and relevant to the current state of the code.\\n\\nBy leveraging these capabilities, Langsmith can help developers and testers save time and resources, improve testing efficiency, and deliver higher-quality software.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"how can langsmith help with testing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021a299a-eedd-4d91-8e49-6ee97096a04e",
   "metadata": {},
   "source": [
    "# LLAMA2 CHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fe9e49c-fc03-44a8-9e7c-ae618693b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_experimental.chat_models import Llama2Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73bd04b0-03ed-4b02-8fa4-8550fb9b756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "template_messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(template_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d083ddd-6e28-462d-9986-bb6d59ab424b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/manasabhat/Models/llama-2-7b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3647.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.14 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.21 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n"
     ]
    }
   ],
   "source": [
    "from os.path import expanduser\n",
    "\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "model_path = expanduser(\"~/Models/llama-2-7b-chat.Q4_0.gguf\")\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    streaming=False,\n",
    ")\n",
    "model = Llama2Chat(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d5b1264-4269-4445-a1ac-1bea44c0fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "chain = LLMChain(llm=model, prompt=prompt_template, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ef94ff5-4e7f-4bd6-84b9-5a3f111a3959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Certainly! Vienna is a beautiful city with many exciting attractions to visit. Here are a few suggestions:\n",
      "1. Sch√∂nbrunn Palace\n",
      "2. St. Stephen's Cathedral\n",
      "3. Hofburg Palace\n",
      "4. Prater Park\n",
      "5. Belvedere Palace\n",
      "6. MuseumsQuartier\n",
      "7. Albertina Museum\n",
      "8. Vienna State Opera\n",
      "9. Burgtheater\n",
      "10. Rathausplatz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    5706.22 ms\n",
      "llama_print_timings:      sample time =       7.73 ms /    96 runs   (    0.08 ms per token, 12417.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7680.84 ms /    47 tokens (  163.42 ms per token,     6.12 tokens per second)\n",
      "llama_print_timings:        eval time =    6570.74 ms /    95 runs   (   69.17 ms per token,    14.46 tokens per second)\n",
      "llama_print_timings:       total time =   14399.00 ms /   142 tokens\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    chain.run(\n",
    "        text=\"What can I see in Vienna? Propose a few locations. Names only, no details.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "061bf6e9-6726-4e24-8369-7ac51cfc5b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course! St. Stephen's Cathedral (Stephansdom in German) is a beautiful Gothic-style cathedral located in the heart of Vienna, Austria. It's one of the most iconic landmarks in the city and a must-visit attraction for anyone interested in history, architecture, or religion. Here are some fun facts about St. Stephen's Cathedral:\n",
      "1. History: The construction of St. Stephen's Cathedral began in the 12th century and took over 600 years to complete. It was built on the site of a former Roman temple dedicated to Jupiter and later a church dedicated to St. Stephen, hence the name.\n",
      "2. Architecture: The cathedral features a unique blend of Gothic and Baroque styles, with intricate stone carvings, stained glass windows, and a soaring central nave. Its towering spires reach 137 meters (450 feet) into the sky and offer stunning views of the city.\n",
      "3. Art and artifacts: St. Stephen's Cathedral houses several works of art and artifacts, including a series of frescoes depicting the lives of the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    5706.22 ms\n",
      "llama_print_timings:      sample time =      21.25 ms /   256 runs   (    0.08 ms per token, 12045.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5660.33 ms /   112 tokens (   50.54 ms per token,    19.79 tokens per second)\n",
      "llama_print_timings:        eval time =   17012.15 ms /   255 runs   (   66.71 ms per token,    14.99 tokens per second)\n",
      "llama_print_timings:       total time =   23125.60 ms /   367 tokens\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(text=\"Tell me more about #2.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bec1833-964f-4fe3-9fd7-a944ae10c7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
